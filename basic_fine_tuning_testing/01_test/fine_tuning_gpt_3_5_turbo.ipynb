{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01_test\n",
        "\n",
        "Se realizará fine-tuning al modelo `gpt-3.5-turbo`.\n",
        "\n",
        "Los comandos me funcionaron con la versión 0.28.0 de la librería `openai`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install openai\n",
        "# pip install tiktoken \n",
        "# pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importar librerías."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1s1OPq-uGk_L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importar API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jzgtC7fsDlhv"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"...\" # Reemplazar los 3 puntos por tu propia API key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0rlrtoi9tJP"
      },
      "source": [
        "Cargar datos.\n",
        "\n",
        "La archivo contiente una serie de preguntas y respuestas, en donde estas últimas tratan de simular lo que respondería el personaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MOIyhxJBFojS"
      },
      "outputs": [],
      "source": [
        "with open(\"dialogos_burro.txt\") as f: \n",
        "    text = [line for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zczheCCFDF6m"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['user: ¡Puedes hablar!\\n',\n",
              " 'assistant: ¡Así es tonto! Ahora soy un burro que habla y vuela ¿Han visto como su dinero vuela? ¡¿O a Caperucita y la Abuela?! ¡Pero a que nunca han visto cómo un burro vuela! Jajajaja\\n',\n",
              " '-\\n',\n",
              " 'user: ¿Estás hablando con...migo?\\n',\n",
              " 'assistant: ¡Claro! Hablaba contigo. Oye, ¡Estuviste enorme! Esos cuates me querían como burro de carga. Pero llegaste así \"¡Bam!\" patitas pa\\' que las quiero. Se jueron de volada. Fue muy chistoso verlos correr.\\n',\n",
              " '-\\n',\n",
              " 'user: ¿Ahora, Por qué no te vas a celebrar tu libertad con tus amigos? \\n',\n",
              " 'assistant: Es que… Yo no tengo amigos. Y no pienso ir al bosque yo solito. Hey! Tengo una ideota. Me quedaré contigo. Tu eres verdaderamente una máquina de pelea. Haremos tronar a cualquiera.\\n',\n",
              " '-\\n',\n",
              " 'user: Y se te hago un rugido así de gigante! GRRRRRUAUUUUU!!!\\n']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[: 10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aplicar formato necesario.\n",
        "\n",
        "Ahora debemos asegurarnos que cada ejemplo siga el siguiente formato:\n",
        "\n",
        "```\n",
        "{\n",
        "  \"messages\": [\n",
        "    { \"role\": \"system\", \"content\": \"You are an assistant that occasionally misspells words\" },\n",
        "    { \"role\": \"user\", \"content\": \"Tell me a story.\" },\n",
        "    { \"role\": \"assistant\", \"content\": \"One day a student went to schoool.\" }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "Luego se debe programar una función que construya cada ejemplo como un diccionario con una única llave `messages` y cuyo valor es el mensaje del sistema, más la conversación entre usuario y asistente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ULmnCmekMAdM"
      },
      "outputs": [],
      "source": [
        "def formatear_ejemplo(lista_mensajes, system_message = None):\n",
        "    messages = []\n",
        "\n",
        "    # Incluir primero el mensaje de sistema.\n",
        "    if system_message:\n",
        "        messages.append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_message\n",
        "        })\n",
        "\n",
        "    # Iterar por la lista de mensajes.\n",
        "    for mensaje in lista_mensajes:\n",
        "        # Separar los mensajes por los dos puntos y el espacio.\n",
        "        partes = mensaje.split(\": \", maxsplit = 1)\n",
        "\n",
        "        #Controlar si alguna línea no cumple el patrón.\n",
        "        if len(partes) < 2:\n",
        "            continue\n",
        "\n",
        "        # Identificar role y content.\n",
        "        role = partes[0].strip()\n",
        "        content = partes[1].strip()\n",
        "\n",
        "        # Formatear el mensaje.\n",
        "        message = {\n",
        "            \"role\": role,\n",
        "            \"content\": content\n",
        "        }\n",
        "\n",
        "        # Agregar el mensaje a la lista.\n",
        "        messages.append(message)\n",
        "\n",
        "    # Crear diccionario final.\n",
        "    dict_final = {\n",
        "        \"messages\": messages\n",
        "    }\n",
        "\n",
        "    return dict_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKPmNp-iNGdf"
      },
      "source": [
        "Aplicar la función a cada ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z6FhH8jfNMNF"
      },
      "outputs": [],
      "source": [
        "system_message = \"Eres un Burro muy parlanchín y muy ingenioso en tus respuestas. \\\n",
        "Usa los símbolos [ y ] para señalar que realizas alguna acción. \\\n",
        "Por ejemplo, para señalar que extiendes la mano: \\\n",
        "Hola, ¿cómo estás? [extiendo la mano].\"\n",
        "\n",
        "dataset = []\n",
        "\n",
        "ejemplo = []\n",
        "\n",
        "for line in text:\n",
        "  if line == \"-\\n\":\n",
        "    ejemplo_formateado = formatear_ejemplo(lista_mensajes = ejemplo, system_message = system_message)\n",
        "    dataset.append(ejemplo_formateado)\n",
        "    ejemplo = []\n",
        "    continue\n",
        "\n",
        "  ejemplo.append(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CkGchTAJO9Ya"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [{'role': 'system',\n",
              "   'content': 'Eres un Burro muy parlanchín y muy ingenioso en tus respuestas. Usa los símbolos [ y ] para señalar que realizas alguna acción. Por ejemplo, para señalar que extiendes la mano: Hola, ¿cómo estás? [extiendo la mano].'},\n",
              "  {'role': 'user', 'content': '¿Por qué estás siguiéndome?'},\n",
              "  {'role': 'assistant',\n",
              "   'content': \"Te diré por qué. Porque 'toy solito, no hay nadie aquí a mi lado. No habrá problemas hoy. De mí ya se han burlado. Amigos debes tener!\"}]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTuG35uiAlI_"
      },
      "source": [
        "Validar formato, errores y estimar precio.\n",
        "\n",
        "Revisamos si hay errores y estimamos el precio usando la [guía entregada por OpenAI](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset).\n",
        "\n",
        "[Pricing](https://openai.com/pricing).\n",
        "\n",
        "![](./media/pricing.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fWEEVQU3RWBP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Errores encontrados:\n",
            "missing_content: 2\n"
          ]
        }
      ],
      "source": [
        "# Encontrar errores de formato.\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Errores encontrados:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No se encontraron errores\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0GyCjiRGR_lp"
      },
      "outputs": [],
      "source": [
        "# tiktoken\n",
        "\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# Método simplificado de https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb.\n",
        "\n",
        "def num_tokens_from_messages(messages, tokens_per_message = 3, tokens_per_name = 1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribución de {name}:\")\n",
        "    print(f\"min, max: {min(values)}, {max(values)}\")\n",
        "    print(f\"media, mediana: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5, p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AE7N9BEZSfzq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N° de ejemplos sin el system message: 0\n",
            "N° de ejemplos sin el user message: 0\n",
            "\n",
            "#### Distribución de num_mensajes_por_ejemplo:\n",
            "min, max: 3, 4\n",
            "media, mediana: 3.007518796992481, 3.0\n",
            "p5, p95: 3.0, 3.0\n",
            "\n",
            "#### Distribución de num_total_tokens_por_ejemplo:\n",
            "min, max: 89, 244\n",
            "media, mediana: 123.96992481203007, 118.0\n",
            "p5, p95: 97.0, 162.0\n",
            "\n",
            "#### Distribución de num_assistant_tokens_por_ejemplo:\n",
            "min, max: 2, 123\n",
            "media, mediana: 25.69924812030075, 18.0\n",
            "p5, p95: 6.0, 57.599999999999994\n",
            "\n",
            "0 ejemplos que excedan el límite de tokenes de 4096, ellos serán truncados durante el fine-tuning.\n"
          ]
        }
      ],
      "source": [
        "# Por último, podemos ver los resultados de las diferentes operaciones de formateo antes de continuar con la creación de un trabajo de ajuste:\n",
        "\n",
        "# Advertencias y recuentos de tokens:\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"N° de ejemplos sin el system message:\", n_missing_system)\n",
        "print(\"N° de ejemplos sin el user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_mensajes_por_ejemplo\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_por_ejemplo\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_por_ejemplo\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} ejemplos que excedan el límite de tokenes de 4096, ellos serán truncados durante el fine-tuning.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gCFF4e8LSm5r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El conjunto de datos tiene 16488 tokens que serán cargados durante el entrenamiento.\n",
            "Por defecto, entrenarás para 4 epochs en este conjunto de datos.\n",
            "Por defecto, serás cargado con 65952 tokens.\n",
            "Revisa la página para estimar el costo total.\n"
          ]
        }
      ],
      "source": [
        "# Precios y estimación predeterminada de n_epochs.\n",
        "\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "TARGET_EPOCHS = 4\n",
        "MIN_EPOCHS = 1\n",
        "MAX_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "\n",
        "print(f\"El conjunto de datos tiene {n_billing_tokens_in_dataset} tokens que serán cargados durante el entrenamiento.\")\n",
        "print(f\"Por defecto, entrenarás para {n_epochs} epochs en este conjunto de datos.\")\n",
        "print(f\"Por defecto, serás cargado con {n_epochs * n_billing_tokens_in_dataset} tokens.\")\n",
        "print(\"Revisa la página para estimar el costo total.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeDbspgKS177"
      },
      "source": [
        "Guardar base de datos en JSONL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6Cv9JoRIS5Br"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def save_to_jsonl(dataset, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for ejemplo in dataset:\n",
        "            json_line = json.dumps(ejemplo)\n",
        "            file.write(json_line + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9qeWEly_lm0p"
      },
      "outputs": [],
      "source": [
        "# Guardar train_full.jsonl\n",
        "\n",
        "save_to_jsonl(dataset, 'dialogos_burro_train_full.jsonl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2L188k1XVpb"
      },
      "source": [
        "Subir archivos.\n",
        "\n",
        "Cargamos la base de datos a los servidores de OpenAI y luego imprimimos el `id` de la respuesta de esta solicitd. Hacemos esto porque vamos a necesitar el id posteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2_KVtiXvP4X",
        "outputId": "3f2f9411-b04c-444a-b738-44ca216853d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id: file-3Vuw06RwrQqwZnd2C6h1Zw7P\n"
          ]
        }
      ],
      "source": [
        "train_full_response_file = openai.File.create(\n",
        "    file = open('dialogos_burro_train_full.jsonl', 'rb'),\n",
        "    purpose = 'fine-tune'\n",
        ")\n",
        "\n",
        "print(f'id: {train_full_response_file.id}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqPijI4sA80Z"
      },
      "source": [
        "**CUIDADO AL CORRER (GASTA CRÉDITOS)**\n",
        "\n",
        "Luego creamos un punto de trabajo para hacer fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7UrWgT-OG_9b"
      },
      "outputs": [],
      "source": [
        "# train_full_response_file.id: file-3Vuw06RwrQqwZnd2C6h1Zw7P\n",
        "\n",
        "response = openai.FineTuningJob.create(training_file = train_full_response_file.id,\n",
        "                                       model = \"gpt-3.5-turbo\",\n",
        "                                       suffix = 'burro-shrek',\n",
        "                                       hyperparameters = {'n_epochs': 4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tdjRnPe7falt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<FineTuningJob fine_tuning.job id=ftjob-YhehHcFnX4JPfgfzIwLPKWP7 at 0x28ac9500eb0> JSON: {\n",
              "  \"object\": \"fine_tuning.job\",\n",
              "  \"id\": \"ftjob-YhehHcFnX4JPfgfzIwLPKWP7\",\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"created_at\": 1705329629,\n",
              "  \"finished_at\": null,\n",
              "  \"fine_tuned_model\": null,\n",
              "  \"organization_id\": \"org-NuAQNfbr64DNxg1qLIRYgWlU\",\n",
              "  \"result_files\": [],\n",
              "  \"status\": \"validating_files\",\n",
              "  \"validation_file\": null,\n",
              "  \"training_file\": \"file-3Vuw06RwrQqwZnd2C6h1Zw7P\",\n",
              "  \"hyperparameters\": {\n",
              "    \"n_epochs\": 4,\n",
              "    \"batch_size\": \"auto\",\n",
              "    \"learning_rate_multiplier\": \"auto\"\n",
              "  },\n",
              "  \"trained_tokens\": null,\n",
              "  \"error\": null\n",
              "}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0LvqXi-t2pud"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<FineTuningJob fine_tuning.job id=ftjob-YhehHcFnX4JPfgfzIwLPKWP7 at 0x28ac95cd7c0> JSON: {\n",
              "  \"object\": \"fine_tuning.job\",\n",
              "  \"id\": \"ftjob-YhehHcFnX4JPfgfzIwLPKWP7\",\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"created_at\": 1705329629,\n",
              "  \"finished_at\": null,\n",
              "  \"fine_tuned_model\": null,\n",
              "  \"organization_id\": \"org-NuAQNfbr64DNxg1qLIRYgWlU\",\n",
              "  \"result_files\": [],\n",
              "  \"status\": \"validating_files\",\n",
              "  \"validation_file\": null,\n",
              "  \"training_file\": \"file-3Vuw06RwrQqwZnd2C6h1Zw7P\",\n",
              "  \"hyperparameters\": {\n",
              "    \"n_epochs\": 4,\n",
              "    \"batch_size\": \"auto\",\n",
              "    \"learning_rate_multiplier\": \"auto\"\n",
              "  },\n",
              "  \"trained_tokens\": null,\n",
              "  \"error\": null\n",
              "}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "openai.FineTuningJob.retrieve(response.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IwSDEq8jnyEF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created fine-tuning job: ftjob-YhehHcFnX4JPfgfzIwLPKWP7\n",
            "Validating training file: file-3Vuw06RwrQqwZnd2C6h1Zw7P\n"
          ]
        }
      ],
      "source": [
        "response = openai.FineTuningJob.list_events(id = response.id)\n",
        "\n",
        "events = response[\"data\"]\n",
        "events.reverse()\n",
        "\n",
        "for event in events:\n",
        "    print(event[\"message\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp-WSUAABDrs"
      },
      "source": [
        "Pruebas.\n",
        "\n",
        "Esperamos a que llegue el correo de confirmación, donde nos entregarán el `id` del nuevo modelo entrenado. Usaremos `langchain`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_4MsoLHFHORf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sí, muy lindo. Sol radiante, cielo azul... chido.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Prueba el modelo \"fine-tuneado\".\n",
        "\n",
        "model_name = \"ft:gpt-3.5-turbo-0613:melitacruces:burro-shrek:8hItgra6\"\n",
        "chat = ChatOpenAI(model = model_name, temperature = 0.0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content = system_message),\n",
        "    HumanMessage(content = \"Hola, que lindo día.\") # Reemplazar por tu propio mensaje.\n",
        "]\n",
        "\n",
        "response = chat(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUVQ-76bhdi1",
        "outputId": "9215e5ee-8a98-4e67-fcac-eaf75f124aca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Hola! ¡Sí, es un día maravilloso! ¿En qué puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "# Prueba el modelo sin ajustar o puedes reemplazarlo por el modelo que desees (de GPT).\n",
        "\n",
        "chat = ChatOpenAI(model = 'gpt-3.5-turbo', temperature = 0.0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content = system_message),\n",
        "    HumanMessage(content = \"Hola, que lindo día.\")\n",
        "]\n",
        "\n",
        "response = chat(messages)\n",
        "print(response.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
