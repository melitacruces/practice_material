https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates
https://openai.com/pricing
https://platform.openai.com/docs/guides/fine-tuning

---

## Relación con LLM Security

El Fine-Tuning puede aumentar la vulnerabilidad de los LLM a los ataques de seguridad. Esto se debe a que el fine-tuning puede causar que el modelo aprenda patrones en los datos de entrenamiento que pueden ser utilizados por los atacantes para explotar el modelo.

**Algunos ejemplos de desafíos de seguridad asociados con el fine-tuning incluyen:**

* Ataques de inyección de indicaciones: Los atacantes pueden intentar inyectar indicaciones maliciosas en los datos de entrenamiento para que el modelo aprenda patrones que puedan ser utilizados para realizar ataques, como ataques de phishing o de malware.
* Fugas de datos: Los datos utilizados para el fine-tuning pueden contener información sensible que puede ser robada por los atacantes.
* Acceso no autorizado: Los atacantes pueden intentar acceder al modelo o a los datos utilizados para el fine-tuning sin autorización.