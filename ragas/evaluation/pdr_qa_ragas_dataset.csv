question,answer,contexts,ground_truths
What does this paper aim to conduct a survey about?,Answer: This paper aims to conduct a survey about retrieval-augmented text generation.,"['2\nRetrieval-Augmented Paradigm\nIn this section, we ﬁrst give a general formulation\nof retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrieval-\naugmented generation paradigm, including the re-\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\nInput\nSources \n(Sec. 2.2):\nTraining \nCorpus\nExternal Data\nUnsupervised \nData\nMetrics\n(Sec. 2.3):\nSparse-vector \nRetrieval\nDense-vector \nRetrieval\nTask-specific \nRetrieval\nRetrieval Memory\nGeneration Model\nSec. 4: Machine \nTranslation\nSec. 5: Other \nTasks\nData \nAugmentation\nAttention \nMechanism\nSkeleton & \nTemplates\nInformation Retrieval\nTasks:\nSec. 3: Dialogue \nGeneration\nModels \n(Sec 2.4):\nOutput\nFigure 1: The overview of this survey.\ntrieval source, retrieval metric and integration meth-\nods.\n2.1\nFormulation\nMost text generation tasks can be formulated as a\nmapping from input sequence x to output sequence\ny : y = f(x). For instance, x and y could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\ny = f(x, z)\n(1)'
 'web knowledge, namely external knowledge.\n5\nExperiments\nWe conducted experiments to extensively demon-\nstrate CRAG’s adaptability to RAG-based ap-\nproaches and its generalizability across both short-\nand long-form generation tasks.\n5.1\nTasks, Datasets and Metrics\nCRAG was evaluated on four datasets, including\nPopQA (Mallen et al., 2023) (short-form gener-\nation), Biography (Min et al., 2023) (long-form\ngeneration), PubHealth (Zhang et al., 2023a) (true-\nor-false question), and Arc-Challenge (Bhaktha-\nvatsalam et al., 2021) (multiple-choice question).\n2In this study, Google Search API is utilized for searching.\nPopQA\nBio\nPub\nARC\nMethod\n(Accuracy)\n(FactScore)\n(Accuracy)\n(Accuracy)\nLMs trained with propriety data\nLLaMA2-c13B\n20.0\n55.9\n49.4\n38.4\nRet-LLaMA2-c13B\n51.8\n79.9\n52.1\n37.9\nChatGPT\n29.3\n71.8\n70.1\n75.3\nRet-ChatGPT\n50.8\n-\n54.7\n75.3\nPerplexity.ai\n-\n71.2\n-\n-\nBaselines without retrieval\nLLaMA27B\n14.7\n44.5\n34.2\n21.8\nAlpaca7B\n23.6\n45.8\n49.8\n45.0\nLLaMA213B\n14.7\n53.4\n29.4\n29.4\nAlpaca13B\n24.4\n50.2\n55.5\n54.9\nCoVE65B\n-\n71.2\n-\n-\nBaselines with retrieval\nLLaMA27B\n38.2\n78.0\n30.0\n48.0\nAlpaca7B\n46.7\n76.6\n40.2\n48.0\nSAIL\n-\n-\n69.2\n48.4\nLLaMA213B\n45.7\n77.5\n30.2\n26.0\nAlpaca13B\n46.1\n77.7\n51.1\n57.6\nLLaMA2-hf-7b\nRAG\n37.7\n44.9\n9.1\n23.8\nCRAG\n39.8\n47.7\n9.1\n25.8\nSelf-RAG*\n29.0\n32.2\n0.7\n23.9\nSelf-CRAG\n49.0\n69.1\n0.6\n27.9\nSelfRAG-LLaMA2-7b\nRAG\n40.3\n59.2\n39.0\n46.7\nCRAG\n59.3\n74.1\n75.6\n54.8\nSelf-RAG\n54.9\n81.2\n72.4\n67.3\nSelf-CRAG\n61.8\n86.2\n74.8\n67.2']","['This paper aims to conduct a survey about retrieval-augmented text generation, highlighting the generic paradigm of this approach, reviewing notable methods across various tasks such as dialogue response generation and machine translation, and identifying important future research directions.']"
What is the main focus of the paper 'A Survey on Retrieval-Augmented Text Generation'?,"Answer: The main focus of the paper is to conduct a survey about retrieval-augmented text generation, highlighting its generic paradigm and reviewing notable approaches for different tasks such as dialogue response generation and machine translation.","['A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,'
 'formance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing'
 'trieval may lead to generation with higher quality\nin the future.\n7\nConclusion\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different com-\nponents of retrieval-augmented text generation in-\ncluding retrieval metrics, retrieval sources, and in-\ntegration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other genera-\ntion tasks. We also pointed out some future direc-\ntions for retrieval-augmented text generation.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1921–1931.\nErgun Biçici and Marc Dymetman. 2008.\nDynamic\ntranslation memory: Using statistical machine trans-\nlation to improve translation memory fuzzy matches.\nIn International Conference on Intelligent Text Pro-\ncessing and Computational Linguistics, pages 454–\n465. Springer.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,']","[""The main focus of the paper 'A Survey on Retrieval-Augmented Text Generation' is to conduct a comprehensive survey on the topic of retrieval-augmented text generation within the field of computational linguistics. It highlights the generic paradigm of retrieval-augmented generation models, reviews notable approaches across various natural language processing (NLP) tasks such as dialogue response generation and machine translation, and discusses the advantages of these models over conventional generation models. Additionally, the paper identifies and suggests important future research directions in this area.""]"
What is the aim of this paper?,The aim of this paper is to propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation in retrieval-augmented text generation tasks.,"['2021) show that CRAG can significantly improve\nthe performance of standard RAG and state-of-the-\nart Self-RAG, demonstrating its generalizability\nacross both short- and long-form generation tasks.\nTo facilitate others to reproduce our results, we will\npublish all source code later.\nIn summary, our contributions in this paper are\nthree-fold: 1) This paper studies the scenarios\nwhere the retriever returns inaccurate results and,\nto the best of our knowledge, makes the first\nattempt to design corrective strategies for RAG to\nimprove its robustness. 2) A plug-and-play method\nnamed CRAG is proposed to improve the ability of\nautomatic self-correction and efficient utilization\nof retrieved documents. 3) Experimental results\nextensively demonstrate CRAG’s adaptability to\nRAG-based approaches and its generalizability\nacross short- and long-form generation tasks.\n2\nRelated Work\nHallucinations of LLMs\nAlthough LLMs have\nexhibited impressive abilities to understand instruc-\ntions and generate fluent language texts (Bang et al.,\n2023; Qin et al., 2023; Zhong et al., 2023), one\nof the most severe issues that LLMs have still\nbeen struggling with is hallucinations. As many\nstudies found (Zhang et al., 2023b; Shuster et al.,\n2021), either outdated information or incorrect\nknowledge that is activated would seriously result\nin hallucinations. Large-scale unregulated training\ndata collection, low proportion of high-quality sam-\npling data, imperfection of data allocation in the'
 'Corrective Retrieval Augmented Generation\nShi-Qi Yan1*, Jia-Chen Gu2*, Yun Zhu3, Zhen-Hua Ling1\n1National Engineering Research Center of Speech and Language Information Processing,\nUniversity of Science and Technology of China, Hefei, China\n2Department of Computer Science, University of California, Los Angeles\n3Google Research\nyansiki@mail.ustc.edu.cn, gujc@ucla.edu, yunzhu@google.com, zhling@ustc.edu.cn\nAbstract\nLarge language models (LLMs) inevitably\nexhibit hallucinations since the accuracy of\ngenerated texts cannot be secured solely by\nthe parametric knowledge they encapsulate. Al-\nthough retrieval-augmented generation (RAG)\nis a practicable complement to LLMs, it relies\nheavily on the relevance of retrieved docu-\nments, raising concerns about how the model\nbehaves if retrieval goes wrong. To this end, we\npropose the Corrective Retrieval Augmented\nGeneration (CRAG) to improve the robustness\nof generation.\nSpecifically, a lightweight\nretrieval evaluator is designed to assess the\noverall quality of retrieved documents for a\nquery, returning a confidence degree based\non which different knowledge retrieval ac-\ntions can be triggered. Since retrieval from\nstatic and limited corpora can only return sub-\noptimal documents, large-scale web searches\nare utilized as an extension for augmenting the\nretrieval results. Besides, a decompose-then-\nrecompose algorithm is designed for retrieved\ndocuments to selectively focus on key infor-\nmation and filter out irrelevant information in'
 'A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,'
 'formance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing']","['The aim of this paper is to conduct a comprehensive survey on retrieval-augmented text generation, highlighting its advantages and state-of-the-art performance in various NLP tasks. It reviews notable approaches and paradigms in the field, including dialogue response generation and machine translation, and suggests important future research directions.']"
What is the aim of this paper?,Answer: The aim of this paper is to propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation in retrieval-augmented text generation tasks.,"['2021) show that CRAG can significantly improve\nthe performance of standard RAG and state-of-the-\nart Self-RAG, demonstrating its generalizability\nacross both short- and long-form generation tasks.\nTo facilitate others to reproduce our results, we will\npublish all source code later.\nIn summary, our contributions in this paper are\nthree-fold: 1) This paper studies the scenarios\nwhere the retriever returns inaccurate results and,\nto the best of our knowledge, makes the first\nattempt to design corrective strategies for RAG to\nimprove its robustness. 2) A plug-and-play method\nnamed CRAG is proposed to improve the ability of\nautomatic self-correction and efficient utilization\nof retrieved documents. 3) Experimental results\nextensively demonstrate CRAG’s adaptability to\nRAG-based approaches and its generalizability\nacross short- and long-form generation tasks.\n2\nRelated Work\nHallucinations of LLMs\nAlthough LLMs have\nexhibited impressive abilities to understand instruc-\ntions and generate fluent language texts (Bang et al.,\n2023; Qin et al., 2023; Zhong et al., 2023), one\nof the most severe issues that LLMs have still\nbeen struggling with is hallucinations. As many\nstudies found (Zhang et al., 2023b; Shuster et al.,\n2021), either outdated information or incorrect\nknowledge that is activated would seriously result\nin hallucinations. Large-scale unregulated training\ndata collection, low proportion of high-quality sam-\npling data, imperfection of data allocation in the'
 'Corrective Retrieval Augmented Generation\nShi-Qi Yan1*, Jia-Chen Gu2*, Yun Zhu3, Zhen-Hua Ling1\n1National Engineering Research Center of Speech and Language Information Processing,\nUniversity of Science and Technology of China, Hefei, China\n2Department of Computer Science, University of California, Los Angeles\n3Google Research\nyansiki@mail.ustc.edu.cn, gujc@ucla.edu, yunzhu@google.com, zhling@ustc.edu.cn\nAbstract\nLarge language models (LLMs) inevitably\nexhibit hallucinations since the accuracy of\ngenerated texts cannot be secured solely by\nthe parametric knowledge they encapsulate. Al-\nthough retrieval-augmented generation (RAG)\nis a practicable complement to LLMs, it relies\nheavily on the relevance of retrieved docu-\nments, raising concerns about how the model\nbehaves if retrieval goes wrong. To this end, we\npropose the Corrective Retrieval Augmented\nGeneration (CRAG) to improve the robustness\nof generation.\nSpecifically, a lightweight\nretrieval evaluator is designed to assess the\noverall quality of retrieved documents for a\nquery, returning a confidence degree based\non which different knowledge retrieval ac-\ntions can be triggered. Since retrieval from\nstatic and limited corpora can only return sub-\noptimal documents, large-scale web searches\nare utilized as an extension for augmenting the\nretrieval results. Besides, a decompose-then-\nrecompose algorithm is designed for retrieved\ndocuments to selectively focus on key infor-\nmation and filter out irrelevant information in'
 'A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,'
 'formance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing']","['The aim of this paper is to conduct a survey about retrieval-augmented text generation, highlighting the generic paradigm of this approach, reviewing notable methods across various tasks such as dialogue response generation and machine translation, and identifying important future research directions.']"
What is the focus of this paper?,The focus of this paper is on Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation by assessing the quality of retrieved documents and triggering different knowledge retrieval actions based on confidence degrees.,"['secure that generative LMs can obtain relevant and\naccurate knowledge. If retrieved documents are\nirrelevant, the retrieval system can even exacerbate\nthe factual error that LMs make.\nAdvanced RAG\nMany advanced approaches\nhave been developed from the original RAG in\nrecent years. Considering that retrieval is some-\ntimes unnecessary for some queries, conversely,\nresponses without retrieval are even more accurate\nin many situations. Self-RAG (Asai et al., 2023)\nis proposed to selectively retrieve knowledge and\nintroduce a critic model to decide whether to\nretrieve. Yoran et al. (2023) designed an NLI model\nto identify the irrelevant context and improve\nrobustness. SAIL (Luo et al., 2023) is tuned on\ninstructions to insert retrieved documents before in-\nstructions. While Toolformer (Schick et al., 2023)\nis pre-trained for calling APIs such as Wikipedia.\nIn addition, in some long-text generation tasks,\nexternal knowledge is needed more than once, and\nwhen to retrieve should be concerned. Jiang et al.\n(2023) actively anticipate future content and decide\nwhen and what to retrieve in long-form generation.\nCompared with recent studies (Schick et al.,\n2023; Luo et al., 2023; Asai et al., 2023) that are\nthe most relevant to our work, a main difference\nshould be highlighted. These approaches target\non exploiting retrieval as a useful tool to augment\ngeneration or whether retrieval is necessary, while\nthis study particularly studies the scenarios where'
 'cation scenario of MoMA.\nEthics Statement\nAll data in this study are publicly available and used\nunder ethical considerations. Text and ﬁgures in the\npaper are used for illustration only, they do not represent\nthe ethical attitude of the authors.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. MS MARCO: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nAlexander Bondarenko, Maik Fröbe, Meriem Be-\nloucif, Lukas Gienapp, Yamen Ajjour, Alexander\nPanchenko, Chris Biemann, Benno Stein, Henning\nWachsmuth, Martin Potthast, and Matthias Hagen.\n2020.\nOverview of Touché 2020: Argument Re-\ntrieval. In Working Notes Papers of the CLEF 2020\nEvaluation Labs, volume 2696 of CEUR Workshop\nProceedings.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206–2240. PMLR.'
 'can, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nICML, pages 2206–2240.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS, pages 1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, and et al. 2022. Palm:\nScaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.'
 'Integrating translation memory into phrase-based\nmachine translation during decoding.\nIn Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 11–21.\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\nDynamically integrating cross-domain translation\nmemory into phrase-based machine translation dur-\ning decoding.\nIn Proceedings of COLING 2014,\nthe 25th International Conference on Computational\nLinguistics: Technical Papers, pages 398–408.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and reﬁne: Improved sequence gen-\neration models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92.\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\njun Li, and Ming Zhou. 2019. Response generation\nby context-aware prototype editing. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 7281–7288.\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\net al. 2021. A controllable model of grounded re-\nsponse generation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, volume 35, pages\n14085–14093.\nMengzhou Xia, Guoping Huang, Lemao Liu, and\nShuming Shi. 2019. Graph based translation mem-\nory for neural machine translation. In Proceedings']","['The focus of this paper is on retrieval-augmented text generation within the field of computational linguistics. It aims to provide a survey of the current state of retrieval-augmented text generation, discussing its advantages, reviewing notable approaches across various NLP tasks such as dialogue response generation and machine translation, and suggesting future research directions.']"
What are the advantages of retrieval-augmented text generation compared to conventional generation models?,Answer: Retrieval-augmented text generation has remarkable advantages and particularly has achieved state-of-the-art performance in many NLP tasks.,"['A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,'
 'trieval may lead to generation with higher quality\nin the future.\n7\nConclusion\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different com-\nponents of retrieval-augmented text generation in-\ncluding retrieval metrics, retrieval sources, and in-\ntegration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other genera-\ntion tasks. We also pointed out some future direc-\ntions for retrieval-augmented text generation.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1921–1931.\nErgun Biçici and Marc Dymetman. 2008.\nDynamic\ntranslation memory: Using statistical machine trans-\nlation to improve translation memory fuzzy matches.\nIn International Conference on Intelligent Text Pro-\ncessing and Computational Linguistics, pages 454–\n465. Springer.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,']","['The advantages of retrieval-augmented text generation compared to conventional generation models include the ability to incorporate external knowledge, which can lead to more informative and contextually relevant outputs. This approach has achieved state-of-the-art performance in various NLP tasks by leveraging retrieved content to enhance the generation process. Additionally, it allows for more diverse and accurate responses in dialogue systems, improved translation quality in machine translation, and overall better performance in text generation tasks by effectively integrating information from relevant sources.']"
What are the advantages of retrieval-augmented text generation compared to conventional generation models?,Answer: Retrieval-augmented text generation has remarkable advantages and particularly has achieved state-of-the-art performance in many NLP tasks.,"['A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,'
 'trieval may lead to generation with higher quality\nin the future.\n7\nConclusion\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different com-\nponents of retrieval-augmented text generation in-\ncluding retrieval metrics, retrieval sources, and in-\ntegration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other genera-\ntion tasks. We also pointed out some future direc-\ntions for retrieval-augmented text generation.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1921–1931.\nErgun Biçici and Marc Dymetman. 2008.\nDynamic\ntranslation memory: Using statistical machine trans-\nlation to improve translation memory fuzzy matches.\nIn International Conference on Intelligent Text Pro-\ncessing and Computational Linguistics, pages 454–\n465. Springer.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,']","['The advantages of retrieval-augmented text generation compared to conventional generation models include the ability to incorporate external knowledge, which can enhance the relevance and richness of the generated text. This approach has achieved state-of-the-art performance in various NLP tasks by leveraging information retrieved from relevant documents or databases. It allows the model to generate more accurate, contextually appropriate, and informative content by grounding the generation process in real-world data. Additionally, retrieval-augmented models can reduce the need for extensive training data, as they can pull from existing resources, and they can be more adaptable to different domains or topics by simply changing the retrieval source.']"
What are the advantages of retrieval-augmented text generation compared to conventional generation models?,Advantages of retrieval-augmented text generation compared to conventional generation models include achieving state-of-the-art performance in many NLP tasks and attracting increasing attention from the computational linguistics community.,"['A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,'
 'trieval may lead to generation with higher quality\nin the future.\n7\nConclusion\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different com-\nponents of retrieval-augmented text generation in-\ncluding retrieval metrics, retrieval sources, and in-\ntegration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other genera-\ntion tasks. We also pointed out some future direc-\ntions for retrieval-augmented text generation.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1921–1931.\nErgun Biçici and Marc Dymetman. 2008.\nDynamic\ntranslation memory: Using statistical machine trans-\nlation to improve translation memory fuzzy matches.\nIn International Conference on Intelligent Text Pro-\ncessing and Computational Linguistics, pages 454–\n465. Springer.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,']","['Retrieval-augmented text generation models have several advantages over conventional generation models. Firstly, they can leverage external knowledge sources to enhance the content quality and relevance, leading to more informative and contextually appropriate outputs. Secondly, these models can achieve state-of-the-art performance in various NLP tasks by integrating retrieved information into the generation process, which helps in grounding the generated text in real-world knowledge. Thirdly, retrieval-augmented approaches can improve the diversity of the generated text, as they can draw from a wide range of sources for inspiration. Lastly, they can reduce the burden on the generative model to store all necessary information, as they can dynamically access relevant data during the generation process.']"
What is the focus of the paper 'A Survey on Retrieval-Augmented Text Generation'?,"Answer: The focus of the paper is on conducting a survey about retrieval-augmented text generation, highlighting the generic paradigm, reviewing notable approaches for different tasks, and pointing out promising directions for future research.","['A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,'
 'formance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing']","[""The focus of the paper 'A Survey on Retrieval-Augmented Text Generation' is to conduct a comprehensive survey about retrieval-augmented text generation in the field of computational linguistics. It highlights the generic paradigm of retrieval-augmented generation, reviews notable approaches across various NLP tasks such as dialogue response generation and machine translation, and discusses the advantages of this method over conventional generation models. Additionally, the paper identifies important future research directions in this area.""]"
